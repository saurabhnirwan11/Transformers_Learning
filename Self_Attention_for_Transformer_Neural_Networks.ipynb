{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gO5Z0qErNuOt"
      },
      "source": [
        "# Self Attention in Transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HedntyUvLrBo"
      },
      "source": [
        "## Generate Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "xtKbaWhFJui3"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import math\n",
        "\n",
        "L, d_k, d_v = 4, 8, 8\n",
        "q = np.random.randn(L, d_k)\n",
        "k = np.random.randn(L, d_k)\n",
        "v = np.random.randn(L, d_v)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "1. q: query vector, k: Key vector, v: Value vector \n",
        "2. L : Length of sequence. In this case our sequence is \"My name is Ajay\" Hence length of sequence is 4.\n",
        "3. d_k, d_v : are the dimensions of k and v vectors respectively.\n",
        "4. d_k = d_v = 8(you can chose any dimension)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "09JpvuNJ2sZC",
        "outputId": "30d2c627-8647-44e0-aa92-c9e53e3b7843"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Q\n",
            " [[-1.36415637  0.59692338  0.81469767 -0.17490486  0.60457722  0.97285416\n",
            "   0.65119482 -0.09519678]\n",
            " [-1.30969525 -0.94479345 -0.3673079  -0.12212057  1.7617716   1.49037266\n",
            "   0.02568805  1.07899398]\n",
            " [ 1.11876351  0.16859557 -0.33980155 -1.04705635 -0.85774411 -0.0475422\n",
            "  -0.25589572  1.22947487]\n",
            " [-0.7141838  -0.01038527  2.44348975 -0.8276257  -0.62324254  0.4149842\n",
            "  -0.47792485  0.06031733]]\n",
            "K\n",
            " [[ 1.9203213   1.14009187  0.19172904  0.75344955 -0.87181049  1.53841683\n",
            "   0.21507366  0.18913319]\n",
            " [ 0.04653665  0.62462718 -1.50235362 -0.42596872  0.06980895  0.34065727\n",
            "  -0.22526977 -1.0466694 ]\n",
            " [ 1.50227701  0.73966022 -0.27537201  0.26149526 -0.53322466 -0.08374696\n",
            "  -0.6863729  -1.31370446]\n",
            " [ 0.04921155 -0.52548654 -0.54641947  0.89833533  1.41068005 -0.77247713\n",
            "   0.60306728  0.69031295]]\n",
            "V\n",
            " [[ 1.02017695  0.96976545  0.61554372  1.69284593  0.71702866 -0.59883186\n",
            "  -0.67487099  0.13410379]\n",
            " [-0.50207642  0.6248769  -0.10653077 -0.18484618 -0.42154398  1.19186699\n",
            "  -0.23122563 -0.6135802 ]\n",
            " [-0.21584725 -1.76280262 -0.09101529  1.2244352  -0.51201264 -2.05087927\n",
            "   0.39329684 -0.30792317]\n",
            " [-0.81534251 -0.94888131 -1.31522167  1.00228874 -1.43690397 -0.5246317\n",
            "  -0.89651149 -0.49520261]]\n"
          ]
        }
      ],
      "source": [
        "print(\"Q\\n\", q)\n",
        "print(\"K\\n\", k)\n",
        "print(\"V\\n\", v)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(4, 8)\n",
            "(4, 8)\n",
            "(4, 8)\n"
          ]
        }
      ],
      "source": [
        "print(q.shape)\n",
        "print(k.shape)\n",
        "print(v.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### shape of the vectors\n",
        "1. 4 >> length of the sequence\n",
        "2. 8 >> each word in the sequence is represented by 8 dimensional vector.\n",
        "3. For every single word we have 8 dimensional q vector, 8 dimensional k vector and 8 dimensional v vector."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tV6txskBLwjh"
      },
      "source": [
        "## Self Attention\n",
        "\n",
        "$$\n",
        "\\text{self attention} = softmax\\bigg(\\frac{Q.K^T}{\\sqrt{d_k}}+M\\bigg)\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\text{new V} = \\text{self attention}.V\n",
        "$$ "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In order to create an intial self attention matrix we need every word to look at every single other word to see if it has a higher effinity towards it or not. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x7GePHKk3Mh0",
        "outputId": "7dae7f5e-4715-4fd4-fbfd-7c0815e7d39e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[-0.82302342, -0.51352891, -2.60365311, -0.55474118],\n",
              "       [-2.78814047, -0.55168962, -5.09648387,  2.61737547],\n",
              "       [ 2.33869455, -0.19139374,  0.6469928 , -1.26734931],\n",
              "       [-0.44800176, -3.21577647, -1.42350222, -3.55468917]])"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "np.matmul(q, k.T)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "q = 4 x 8 and k.T = 8 x 4 Hence the result is 4 x4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "1. So each number is telling me the attention that each word will give to the other word.\n",
        "2. second number[0,1] in the array is telling me how much \"My\" is focusing on \"Name\"."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![](Data/attention_matrix.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "odK76OoI3nL2",
        "outputId": "69b50cdb-9a41-45ae-bfd2-619228af1ef7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(0.8189305952168686, 0.7046191727529998, 3.9175175362098447)"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Why we need sqrt(d_k) in denominator\n",
        "q.var(), k.var(), np.matmul(q, k.T).var()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Answer: To minimise the variance of `np.matmul(q, k.T)`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Ps6AY1Q3tRI",
        "outputId": "3b9ac3c8-70b8-47bd-e868-e7d6fd26d270"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(0.8189305952168686, 0.7046191727529998, 0.4896896920262304)"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "scaled = np.matmul(q, k.T) / math.sqrt(d_k)\n",
        "q.var(), k.var(), scaled.var()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ypO9IK1PL3cJ"
      },
      "source": [
        "Notice the reduction in variance of the product"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LVHAJR4N4VQX",
        "outputId": "52b06cf8-0381-453c-b576-0bd8de9a38b5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[-0.29098272, -0.18155989, -0.92053039, -0.19613062],\n",
              "       [-0.98575652, -0.19505174, -1.80187915,  0.92538197],\n",
              "       [ 0.82685339, -0.06766791,  0.2287465 , -0.44807565],\n",
              "       [-0.15839254, -1.13694867, -0.50328404, -1.25677241]])"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "scaled"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![](Data/attention_matrix.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dmz4v-RmMAaj"
      },
      "source": [
        "## Masking\n",
        "\n",
        "- This is to ensure words don't get context from words generated in the future. \n",
        "- Not required in the encoders, but required in the decoders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Masking is done so that the netowrk dont look at a future word when generating current word.\n",
        "\n",
        "\"We dont look at future word while generating the current context of the current word\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e8N3OhSLILfG",
        "outputId": "2c63a444-066c-44b2-abe5-242dd989f311"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[1., 0., 0., 0.],\n",
              "       [1., 1., 0., 0.],\n",
              "       [1., 1., 1., 0.],\n",
              "       [1., 1., 1., 1.]])"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "mask = np.tril(np.ones( (L, L) ))\n",
        "mask"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* All of the values below the diagnol is one and above the diagnol is zero.\n",
        "* This will simulate the fact that mentioned above\n",
        "\n",
        "|      | My | Name| is |Ajay|\n",
        "|------|----|-----|----|----|\n",
        "| My   | 1  | 0   | 0  | 0  |\n",
        "| Name | 1  | 1   | 0  | 0  |\n",
        "| is   | 1  | 1   | 1  | 0  |\n",
        "| Ajay | 1  | 1   | 1  | 1  |\n",
        "\n",
        "* \"My\" can look at only \"My\". while generating the first word\n",
        "* \"Name\" can look at \"My\" and \"Name\" \n",
        "* \"Ajay\" can look at \"My\", \"Name\", \"is\", \"Ajay\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "hIV9K3Yn6s1V"
      },
      "outputs": [],
      "source": [
        "mask[mask == 0] = -np.infty\n",
        "mask[mask == 1] = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LK5V_T3W6vpX",
        "outputId": "bb4160a1-a011-4850-e403-9cb252572c66"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[  0., -inf, -inf, -inf],\n",
              "       [  0.,   0., -inf, -inf],\n",
              "       [  0.,   0.,   0., -inf],\n",
              "       [  0.,   0.,   0.,   0.]])"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "mask"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This transformation of 0 >> - infinty and 1 >> 0 is required for softmax"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lNH1VgEf7xTa",
        "outputId": "4211c411-0356-4e39-8388-d39b0c1d0920"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[-0.29098272,        -inf,        -inf,        -inf],\n",
              "       [-0.98575652, -0.19505174,        -inf,        -inf],\n",
              "       [ 0.82685339, -0.06766791,  0.2287465 ,        -inf],\n",
              "       [-0.15839254, -1.13694867, -0.50328404, -1.25677241]])"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Applying the mask on the scaled output\n",
        "scaled + mask"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- inf means we are not getting any context from future words while generating the current word."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XMTAXjooN9eZ"
      },
      "source": [
        "## Softmax\n",
        "\n",
        "$$\n",
        "\\text{softmax} = \\frac{e^{x_i}}{\\sum_j e^x_j}\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "2R4gdRqj8W4Y"
      },
      "outputs": [],
      "source": [
        "def softmax(x):\n",
        "  return (np.exp(x).T / np.sum(np.exp(x), axis=-1)).T"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "K5eg2zPy41sP"
      },
      "outputs": [],
      "source": [
        "attention = softmax(scaled + mask)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7sauNmfl-1TB",
        "outputId": "46b22beb-9034-4c7c-8d56-04209d2581c4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[1.        , 0.        , 0.        , 0.        ],\n",
              "       [0.31201736, 0.68798264, 0.        , 0.        ],\n",
              "       [0.51055448, 0.20871633, 0.28072919, 0.        ],\n",
              "       [0.41363996, 0.15546798, 0.29298003, 0.13791204]])"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BAy37go56LZo",
        "outputId": "78d97fa1-e0b3-4c1d-8294-bf0fdb77f199"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[ 1.02017695,  0.96976545,  0.61554372,  1.69284593,  0.71702866,\n",
              "        -0.59883186, -0.67487099,  0.13410379],\n",
              "       [-0.02710694,  0.73248812,  0.11876901,  0.40102635, -0.06628955,\n",
              "         0.63313787, -0.36965068, -0.38028982],\n",
              "       [ 0.35546974,  0.13066996,  0.26648324,  1.16944436,  0.13436219,\n",
              "        -0.63271586, -0.28240907, -0.14603994],\n",
              "       [ 0.16824467, -0.14904602,  0.03000079,  1.16845379, -0.11712071,\n",
              "        -0.73562332, -0.3235134 , -0.19843113]])"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "new_v = np.matmul(attention, v)\n",
        "new_v\n",
        "# after attention"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This new_v will be much aware of the context"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vCP2aZOU9VrT",
        "outputId": "e1fe2137-cd95-4a4b-fa1a-3ec21c38104c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[-0.00368231,  1.43739233, -0.59614565, -1.23171219,  1.12030717,\n",
              "        -0.98620738, -0.15461465, -1.03106383],\n",
              "       [ 0.85585446, -1.79878344,  0.67321704,  0.05607552, -0.15542661,\n",
              "        -1.41264124, -0.40136933, -1.17626611],\n",
              "       [ 0.50465335,  2.28693419,  0.67128338,  0.2506863 ,  1.78802234,\n",
              "         0.14775751, -0.11405725,  0.88026286],\n",
              "       [-0.68069105,  0.68385101,  0.17994557, -1.68013201,  0.91543969,\n",
              "        -0.19108312,  0.03160471,  1.40527326]])"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# before attention\n",
        "v"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h_JndWelLDNW"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nSiJuBQELFHT"
      },
      "source": [
        "# Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "XvTnmdcB_jdq"
      },
      "outputs": [],
      "source": [
        "def softmax(x):\n",
        "  return (np.exp(x).T / np.sum(np.exp(x), axis=-1)).T\n",
        "\n",
        "def scaled_dot_product_attention(q, k, v, mask=None):\n",
        "  d_k = q.shape[-1]\n",
        "  scaled = np.matmul(q, k.T) / math.sqrt(d_k)\n",
        "  if mask is not None:\n",
        "    scaled = scaled + mask\n",
        "  attention = softmax(scaled)\n",
        "  out = np.matmul(attention, v)\n",
        "  return out, attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Q\n",
            " [[-1.36415637  0.59692338  0.81469767 -0.17490486  0.60457722  0.97285416\n",
            "   0.65119482 -0.09519678]\n",
            " [-1.30969525 -0.94479345 -0.3673079  -0.12212057  1.7617716   1.49037266\n",
            "   0.02568805  1.07899398]\n",
            " [ 1.11876351  0.16859557 -0.33980155 -1.04705635 -0.85774411 -0.0475422\n",
            "  -0.25589572  1.22947487]\n",
            " [-0.7141838  -0.01038527  2.44348975 -0.8276257  -0.62324254  0.4149842\n",
            "  -0.47792485  0.06031733]]\n",
            "K\n",
            " [[ 1.9203213   1.14009187  0.19172904  0.75344955 -0.87181049  1.53841683\n",
            "   0.21507366  0.18913319]\n",
            " [ 0.04653665  0.62462718 -1.50235362 -0.42596872  0.06980895  0.34065727\n",
            "  -0.22526977 -1.0466694 ]\n",
            " [ 1.50227701  0.73966022 -0.27537201  0.26149526 -0.53322466 -0.08374696\n",
            "  -0.6863729  -1.31370446]\n",
            " [ 0.04921155 -0.52548654 -0.54641947  0.89833533  1.41068005 -0.77247713\n",
            "   0.60306728  0.69031295]]\n",
            "V\n",
            " [[ 1.02017695  0.96976545  0.61554372  1.69284593  0.71702866 -0.59883186\n",
            "  -0.67487099  0.13410379]\n",
            " [-0.50207642  0.6248769  -0.10653077 -0.18484618 -0.42154398  1.19186699\n",
            "  -0.23122563 -0.6135802 ]\n",
            " [-0.21584725 -1.76280262 -0.09101529  1.2244352  -0.51201264 -2.05087927\n",
            "   0.39329684 -0.30792317]\n",
            " [-0.81534251 -0.94888131 -1.31522167  1.00228874 -1.43690397 -0.5246317\n",
            "  -0.89651149 -0.49520261]]\n",
            "New V\n",
            " [[-0.14712874 -0.0842247  -0.26624671  0.86475164 -0.42848548 -0.2504696\n",
            "  -0.45597668 -0.33590836]\n",
            " [-0.54714961 -0.46570527 -0.8216394   0.82657606 -0.97555284 -0.23295117\n",
            "  -0.67947949 -0.451861  ]\n",
            " [ 0.20928164 -0.00412326  0.06899091  1.14857324 -0.06182676 -0.61922042\n",
            "  -0.35908616 -0.18963653]\n",
            " [ 0.16824467 -0.14904602  0.03000079  1.16845379 -0.11712071 -0.73562332\n",
            "  -0.3235134  -0.19843113]]\n",
            "Attention\n",
            " [[0.2668116  0.29766409 0.14216596 0.29335835]\n",
            " [0.0960811  0.21185401 0.04248156 0.64958333]\n",
            " [0.44680644 0.18265592 0.24567723 0.12486042]\n",
            " [0.41363996 0.15546798 0.29298003 0.13791204]]\n"
          ]
        }
      ],
      "source": [
        "# Encoder\n",
        "values, attention = scaled_dot_product_attention(q, k, v, mask=None)\n",
        "print(\"Q\\n\", q)\n",
        "print(\"K\\n\", k)\n",
        "print(\"V\\n\", v)\n",
        "print(\"New V\\n\", values)\n",
        "print(\"Attention\\n\", attention)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "`Attention\n",
        " [[0.2668116  0.29766409 0.14216596 0.29335835]\n",
        " [0.0960811  0.21185401 0.04248156 0.64958333]\n",
        " [0.44680644 0.18265592 0.24567723 0.12486042]\n",
        " [0.41363996 0.15546798 0.29298003 0.13791204]]`\n",
        "\n",
        " Notice that the every word here is looking at every word wheras below is not that case."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KSxLkZdiSLMT",
        "outputId": "ca70508d-fb6e-4eec-acb6-7a89a60dffa8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Q\n",
            " [[-1.36415637  0.59692338  0.81469767 -0.17490486  0.60457722  0.97285416\n",
            "   0.65119482 -0.09519678]\n",
            " [-1.30969525 -0.94479345 -0.3673079  -0.12212057  1.7617716   1.49037266\n",
            "   0.02568805  1.07899398]\n",
            " [ 1.11876351  0.16859557 -0.33980155 -1.04705635 -0.85774411 -0.0475422\n",
            "  -0.25589572  1.22947487]\n",
            " [-0.7141838  -0.01038527  2.44348975 -0.8276257  -0.62324254  0.4149842\n",
            "  -0.47792485  0.06031733]]\n",
            "K\n",
            " [[ 1.9203213   1.14009187  0.19172904  0.75344955 -0.87181049  1.53841683\n",
            "   0.21507366  0.18913319]\n",
            " [ 0.04653665  0.62462718 -1.50235362 -0.42596872  0.06980895  0.34065727\n",
            "  -0.22526977 -1.0466694 ]\n",
            " [ 1.50227701  0.73966022 -0.27537201  0.26149526 -0.53322466 -0.08374696\n",
            "  -0.6863729  -1.31370446]\n",
            " [ 0.04921155 -0.52548654 -0.54641947  0.89833533  1.41068005 -0.77247713\n",
            "   0.60306728  0.69031295]]\n",
            "V\n",
            " [[ 1.02017695  0.96976545  0.61554372  1.69284593  0.71702866 -0.59883186\n",
            "  -0.67487099  0.13410379]\n",
            " [-0.50207642  0.6248769  -0.10653077 -0.18484618 -0.42154398  1.19186699\n",
            "  -0.23122563 -0.6135802 ]\n",
            " [-0.21584725 -1.76280262 -0.09101529  1.2244352  -0.51201264 -2.05087927\n",
            "   0.39329684 -0.30792317]\n",
            " [-0.81534251 -0.94888131 -1.31522167  1.00228874 -1.43690397 -0.5246317\n",
            "  -0.89651149 -0.49520261]]\n",
            "New V\n",
            " [[ 1.02017695  0.96976545  0.61554372  1.69284593  0.71702866 -0.59883186\n",
            "  -0.67487099  0.13410379]\n",
            " [-0.02710694  0.73248812  0.11876901  0.40102635 -0.06628955  0.63313787\n",
            "  -0.36965068 -0.38028982]\n",
            " [ 0.35546974  0.13066996  0.26648324  1.16944436  0.13436219 -0.63271586\n",
            "  -0.28240907 -0.14603994]\n",
            " [ 0.16824467 -0.14904602  0.03000079  1.16845379 -0.11712071 -0.73562332\n",
            "  -0.3235134  -0.19843113]]\n",
            "Attention\n",
            " [[1.         0.         0.         0.        ]\n",
            " [0.31201736 0.68798264 0.         0.        ]\n",
            " [0.51055448 0.20871633 0.28072919 0.        ]\n",
            " [0.41363996 0.15546798 0.29298003 0.13791204]]\n"
          ]
        }
      ],
      "source": [
        "# Decoder\n",
        "values, attention = scaled_dot_product_attention(q, k, v, mask=mask)\n",
        "print(\"Q\\n\", q)\n",
        "print(\"K\\n\", k)\n",
        "print(\"V\\n\", v)\n",
        "print(\"New V\\n\", values)\n",
        "print(\"Attention\\n\", attention)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-HtQQtB2LJus"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
